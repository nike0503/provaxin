{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ ! -d \"ember\" ] && git clone https://github.com/elastic/ember && cd ember && pip install -r requirements.txt && python setup.py install\n",
    "\n",
    "# Restart kernel now to use ember package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5OsZghgXLtPZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter any data directory where you want to store all the data that will be downloaded and models that will be generated\n",
    "main_data_dir = \"ember/data\"\n",
    "if not os.path.exists(main_data_dir):\n",
    "    os.makedirs(main_data_dir)\n",
    "data_dir = \"ember/data/ember2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT\n",
    "## It will download dataset if not already downloaded. The dataset is of size ~10GB and may take a lot of time to get downloaded.\n",
    "\n",
    "import requests, tarfile\n",
    "if not os.path.exists(os.path.join(data_dir, \"X_train.dat\")):\n",
    "    dataset_link = \"https://ember.elastic.co/ember_dataset_2018_2.tar.bz2\"\n",
    "    target_file = os.path.join(os.path.dirname(data_dir), \"ember_dataset_2018_2.tar.bz2\")\n",
    "    res = requests.get(dataset_link, stream=True)\n",
    "    if res.status_code == 200:\n",
    "        with open(target_file, 'wb') as f:\n",
    "            f.write(res.raw.read())\n",
    "        \n",
    "        tar = tarfile.open(target_file, \"r:bz2\")\n",
    "        tar.extractall(os.path.dirname(data_dir))\n",
    "        tar.close()\n",
    "\n",
    "        ember.create_vectorized_features(data_dir)\n",
    "        _ = ember.create_metadata(data_dir)\n",
    "    else:\n",
    "        print(\"Unable to download zip data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-AzEhS9nLL20"
   },
   "outputs": [],
   "source": [
    "# Reading generated vectorized features stored in .dat files as numpy memmap object to efficiently read large file.\n",
    "\n",
    "def read_vectorized_features(data_dir, subset=None, feature_version=2):\n",
    "    \"\"\"\n",
    "    Read vectorized features into memory mapped numpy arrays\n",
    "    \"\"\"\n",
    "    if subset is not None and subset not in [\"train\", \"test\"]:\n",
    "        return None\n",
    "\n",
    "    ndim = 2381\n",
    "    X_train = None\n",
    "    y_train = None\n",
    "    X_test = None\n",
    "    y_test = None\n",
    "\n",
    "    if subset is None or subset == \"train\":\n",
    "        X_train_path = os.path.join(data_dir, \"X_train.dat\")\n",
    "        y_train_path = os.path.join(data_dir, \"y_train.dat\")\n",
    "        y_train = np.memmap(y_train_path, dtype=np.float32, mode=\"r\")\n",
    "        N = y_train.shape[0]\n",
    "        X_train = np.memmap(X_train_path, dtype=np.float32, mode=\"r\", shape=(N, ndim))\n",
    "        if subset == \"train\":\n",
    "            return X_train, y_train\n",
    "\n",
    "    if subset is None or subset == \"test\":\n",
    "        X_test_path = os.path.join(data_dir, \"X_test.dat\")\n",
    "        y_test_path = os.path.join(data_dir, \"y_test.dat\")\n",
    "        y_test = np.memmap(y_test_path, dtype=np.float32, mode=\"r\")\n",
    "        N = y_test.shape[0]\n",
    "        X_test = np.memmap(X_test_path, dtype=np.float32, mode=\"r\", shape=(N, ndim))\n",
    "        if subset == \"test\":\n",
    "            return X_test, y_test\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2Vd_HRj4_Xfj"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = read_vectorized_features(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Z12Gv-6WabV",
    "outputId": "3b68d9b6-5123-4f7a-bac5-9d4fa1c95a4a"
   },
   "outputs": [],
   "source": [
    "# Removing data whose label is unknown\n",
    "\n",
    "train_rows = (Y_train != -1)\n",
    "X_train = X_train[train_rows]\n",
    "Y_train = Y_train[train_rows]\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4N_nwIVCKCov",
    "outputId": "743c552b-1231-46fa-c7df-480d4ee4e177"
   },
   "outputs": [],
   "source": [
    "# Randomly selecting total_rows/r from training dataset to reduce load on training.\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "r = 3\n",
    "\n",
    "do_shuffle = True\n",
    "if not do_shuffle:\n",
    "    X_train , Y_train = X_train[:X_train.shape[0]//r], Y_train[:X_train.shape[0]//r]\n",
    "else:\n",
    "    X_train, Y_train = shuffle(X_train, Y_train, n_samples=len(X_train)//r)\n",
    "    X_test, Y_test = shuffle(X_test, Y_test, n_samples=len(X_test)//r)\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    \"ridge\": lambda X_train, Y_train: RidgeClassifier().fit(X_train, Y_train),\n",
    "    \"K-neighbour\": lambda X_train, Y_train: KNeighborsClassifier(1).fit(X_train, Y_train),\n",
    "    \"SVC\": lambda X_train, Y_train: SVC(gamma=2, C=1).fit(X_train, Y_train),\n",
    "    \"GPC\": lambda X_train, Y_train: GaussianProcessClassifier(1.0 * RBF(1.0)).fit(X_train, Y_train),\n",
    "    \"Decision Tree\": lambda X_train, Y_train: DecisionTreeClassifier(max_depth=10).fit(X_train, Y_train),\n",
    "    \"Random Forest\": lambda X_train, Y_train: RandomForestClassifier(max_depth=20, n_estimators=200).fit(X_train, Y_train),\n",
    "    \"MLP\": lambda X_train, Y_train: MLPClassifier(alpha=1, max_iter=100).fit(X_train, Y_train),\n",
    "    \"ADA Boost\": lambda X_train, Y_train: AdaBoostClassifier().fit(X_train, Y_train),\n",
    "    \"Gaussian Naive Bayesian\": lambda X_train, Y_train: AdaBoostClassifier().fit(X_train, Y_train),\n",
    "    \"Quadratic Discriminant Analysis\": lambda X_train, Y_train: QuadraticDiscriminantAnalysis().fit(X_train, Y_train)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clfs = {}\n",
    "best_clfs_num = 3\n",
    "\n",
    "def add_best_clf(name, clf, key_param):\n",
    "    kp = key_param(clf)\n",
    "    print(f\"model: {name}, score: {kp}\")\n",
    "    clf[\"kp\"] = kp\n",
    "    if len(best_clfs) < best_clfs_num:\n",
    "        best_clfs[name] = clf\n",
    "    else:\n",
    "        change_clf = None\n",
    "        for best_clf_name, best_clf_val in best_clfs.items():\n",
    "            if key_param(best_clf_val) < kp:\n",
    "                change_clf = best_clf_name\n",
    "        \n",
    "        if change_clf is not None:\n",
    "            del best_clfs[change_clf]\n",
    "            best_clfs[name] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6OdiKpQQao_"
   },
   "outputs": [],
   "source": [
    "## Running different models for the dataset. For large data it will take a lot of time and ram. Might give error if computer does not have adequate resources.\n",
    "\n",
    "for clf_name, clf in clfs.items():\n",
    "    model = clf(X_train, Y_train)\n",
    "    score = model.score(X_test, Y_test)\n",
    "    add_best_clf(clf_name, {\n",
    "        \"model\": model,\n",
    "        \"score\": score\n",
    "    },\n",
    "    lambda clf_dict: clf_dict[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crQpspsThIWT"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_model = True\n",
    "save_model_dir = os.path.join(data_dir, \"best_models\")\n",
    "\n",
    "if not os.path.exists(save_model_dir) and save_model:\n",
    "    os.makedirs(save_model_dir)\n",
    "\n",
    "print(\"Best Classifiers are: \")\n",
    "for clf_name, clf in best_clfs.items():\n",
    "    print(f\"classifier: {clf_name}, score: {clf['kp']}\")\n",
    "\n",
    "    if save_model:\n",
    "        pickle.dump(clf[\"model\"], open(os.path.join(save_model_dir, clf_name), 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ember.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
